# ğŸ“˜ Handbook & Quiz Generator

This is a full-stack AI-powered application that allows users to upload documents (such as resumes or training content), from which it generates a structured **training handbook** and **multiple-choice quiz questions** using LLaMA 2. The system is built with **FastAPI (Python)** for the backend and **React (TypeScript)** for the frontend.

## ğŸ§© Features

- ğŸ“„ Upload one or more PDF/text documents.
- ğŸ§  LLaMA 2-based summarization and question generation.
- ğŸ“˜ Outputs a clean, structured training handbook.
- â“ Generates 5 multiple-choice questions with support for **multiple correct answers**.
- ğŸŒ React-based frontend with drag-and-drop UX.
- âœ… Supports semantic chunking and clustering for optimal processing.

## ğŸ—ï¸ Tech Stack

- **Backend**: FastAPI, Python, llama.cpp (via `llama_cpp`)
- **Frontend**: React + TypeScript
- **Model**: `llama-2-7b-chat.Q4_K_M.gguf` (local inference)

## ğŸ“ Folder Structure

```
app/
â”œâ”€â”€ main.py              # FastAPI app entrypoint
â”œâ”€â”€ upload.py            # API route: handles file upload, handbook creation, and quiz generation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ extract.py       # Text extraction from uploaded files
â”‚   â”œâ”€â”€ clean.py         # Cleaning text (remove metadata, whitespace, etc.)
â”‚   â”œâ”€â”€ chunker.py       # Chunk and cluster text semantically
â”‚   â”œâ”€â”€ clusterer.py     # Clustering (agglomerative) logic
â”‚   â”œâ”€â”€ embedder.py      # Text embeddings for semantic operations
â”œâ”€â”€ llama_runner.py      # Run LLaMA chat completions locally
```

## ğŸš€ Getting Started

### 1. Backend Setup

#### âœ… Prerequisites

- Python 3.10+
- Install llama.cpp with Python bindings

```bash
pip install -r requirements.txt
```

#### â–¶ï¸ Run the backend

```bash
uvicorn app.main:app --reload
```

> Make sure the model file `llama-2-7b-chat.Q4_K_M.gguf` is placed in `./app/models/`.

### 2. Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

> The frontend communicates with `localhost:8000`.

## ğŸ“¦ API: `/upload`

**Method**: `POST`  
**Params**:

- `files`: list of PDF or text files
- `prompt`: user prompt (optional)

**Returns**:
```json
{
  "handbuch": "...",
  "quiz": [
    {
      "question": "...",
      "answers": ["...", "...", "...", "..."],
      "right_answers": [1, 2]
    }
  ]
}
```

## ğŸ§  LLaMA Prompting

Prompt chaining is used in these stages:

1. **Summarize** raw document content per semantic chunk.
2. **Format** those summaries into handbook sections.
3. **Generate quiz**: multiple-choice questions using clean JSON schema.

## ğŸ Troubleshooting

- ğŸŸ¥ **Not getting quiz questions?**  
  Ensure at least 3â€“5 rich content chunks are extractable and the model responses are parsable JSON.
  
- ğŸ“„ **Empty handbook?**  
  Check that the uploaded file isnâ€™t an image-based (non-OCR) PDF.

## ğŸ“œ License

MIT